{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN9B9zznDirMKoM9d1pBjaH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vmattey/SeparablePINN_AC_Codes/blob/main/SPINN_AC_DeepRitz.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MoDRpWd0IWdR"
      },
      "outputs": [],
      "source": [
        "# Code for Separable PINN in PyTorch\n",
        "# Solving 1D Allen Cahn Equation\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import TensorDataset\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.func as ft\n",
        "import numpy as np\n",
        "import time\n",
        "import torch.jit as jit\n",
        "import scipy\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from torch.autograd import Variable\n",
        "# Seed for randomization\n",
        "SEED = 444\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##############################################################\n",
        "# Neural Network definitions\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_size, hidden_sizes, output_size, activation='gelu'):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_sizes = hidden_sizes\n",
        "        self.output_size = output_size\n",
        "\n",
        "        # Create a list of hidden layers based on user-defined sizes\n",
        "        self.hidden_layers = nn.ModuleList()\n",
        "        prev_size = input_size  # Initialize the input size\n",
        "        for size in hidden_sizes:\n",
        "            self.hidden_layers.append(nn.Linear(prev_size, size))\n",
        "            prev_size = size\n",
        "\n",
        "        if activation == 'tanh':\n",
        "            self.act_fun = nn.Tanh()\n",
        "        else:\n",
        "            self.act_fun = nn.LeakyReLU()\n",
        "\n",
        "        self.output_layer = nn.Linear(hidden_sizes[-1], output_size)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, X):\n",
        "        for layer in self.hidden_layers:\n",
        "            X = self.act_fun(layer(X))\n",
        "        X = self.output_layer(X)\n",
        "        return X\n",
        "\n",
        "class Combined(nn.Module):\n",
        "    def __init__(self, input_size, hidden_sizes, output_size, activation):\n",
        "        super(Combined, self).__init__()\n",
        "        self.model1 = NeuralNetwork(input_size, hidden_sizes, output_size, activation)\n",
        "        self.model2 = NeuralNetwork(input_size, hidden_sizes, output_size, activation)\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        scaled_x = x\n",
        "        scaled_t = t\n",
        "        model1_output = self.model1(scaled_x)\n",
        "        model2_output = self.model2(scaled_t)\n",
        "        u = torch.matmul(model1_output, model2_output.T)\n",
        "        return u\n"
      ],
      "metadata": {
        "id": "QnGXhRq_Ie_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##############################################################\n",
        "# Auxillary Functions\n",
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        torch.nn.init.xavier_normal_(m.weight)\n",
        "\n",
        "\n",
        "def hvp_fwdfwd(f, primals, tangents, return_primals=False):\n",
        "    g = lambda primals: ft.jvp(f, (primals,), (tangents,))[1]\n",
        "    primals_out, tangents_out = ft.jvp(g, (primals,), (tangents,))\n",
        "    if return_primals:\n",
        "        return primals_out, tangents_out\n",
        "    else:\n",
        "        return tangents_out"
      ],
      "metadata": {
        "id": "ymG2VHSWIo7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##############################################################\n",
        "# Loss Functions\n",
        "\n",
        "def spinn_loss(apply_fn, ad_fn, tau, train_data, train_data_ic):\n",
        "\n",
        "    def residual_loss(x,t):\n",
        "        # calculate u\n",
        "        u = apply_fn(x,t)\n",
        "        # tangent vector dx/dx\n",
        "        # assumes t, x, y have same shape (very important)\n",
        "        v = torch.ones(x.shape)\n",
        "        # 2nd derivatives of u\n",
        "        ux,uxx = hvp_fwdfwd(lambda x: ad_fn(x,t), x, v,return_primals=True)\n",
        "        ut = ft.jvp(lambda t: ad_fn(x,t), (t,), (v,))[1]\n",
        "        #return torch.mean((ut-0.0001*uxx+5*(u**3-u))**2) - 1e-6*(torch.mean(torch.log10(u**2 + ux**2 + uxx**2)) + torch.mean(torch.log10((u-1)**2 + ux**2 + uxx**2))  + torch.mean(torch.log10((u+1)**2 + ux**2 + uxx**2)))\n",
        "        return torch.mean((ut-0.0001*uxx+5*(u**3-u))**2)\n",
        "\n",
        "\n",
        "    def initial_loss(x,t,u):\n",
        "        return torch.mean((apply_fn(x,t) - u)**2)\n",
        "\n",
        "    def moving_loss(x,u,tau,h):\n",
        "        return torch.sum(h*(apply_fn(x) - u)**2)/(2*tau)\n",
        "\n",
        "    def boundary_loss(x):\n",
        "\n",
        "        loss_u = torch.mean((apply_fn(x[0]) - apply_fn(x[1]))**2)\n",
        "        v = torch.ones(x[0].shape)\n",
        "        ux_lb =  ft.jvp(lambda x: ad_fn(x), (x[0],), (v,))[1]\n",
        "        ux_ub =  ft.jvp(lambda x: ad_fn(x), (x[1],), (v,))[1]\n",
        "        loss_ux = torch.mean((ux_lb - ux_ub)**2)\n",
        "        return loss_u + loss_ux\n",
        "\n",
        "    def energy_loss(x, h):\n",
        "        u = apply_fn(x)\n",
        "        v = torch.ones(x.shape)\n",
        "        ux =  ft.jvp(lambda x: ad_fn(x), (x,), (v,))[1]\n",
        "        f = h*(0.5*ux**2 + 12500*(u**2 - 1)**2)\n",
        "        #f = h*(0.5*u**2)\n",
        "        return torch.sum(f)\n",
        "\n",
        "    # unpack data\n",
        "    xg, xb, h, _ = train_data\n",
        "    xi, ui = train_data_ic\n",
        "    ngpt = xg.size()[0]\n",
        "    # Computing the loss value\n",
        "    #res_loss = residual_loss(xc,tc)\n",
        "    moving_loss = moving_loss(xi, ui, tau, h)\n",
        "    ener_loss = energy_loss(xg, h)\n",
        "    bound_loss = boundary_loss(xb)\n",
        "    loss = ener_loss + ngpt*bound_loss + moving_loss\n",
        "\n",
        "    return loss.to(device), ener_loss, bound_loss, moving_loss\n",
        "\n",
        "def icgl_loss(apply_fn, train_data_icgl):\n",
        "    x, u = train_data_icgl\n",
        "    loss = torch.mean((apply_fn(x) - u)**2)\n",
        "    return loss.to(device)"
      ],
      "metadata": {
        "id": "BLMWRnVoiYMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##############################################################\n",
        "# Generating the training data\n",
        "\n",
        "def spinn_train_generator_AC1D(nc, keys):\n",
        "\n",
        "    # collocation points\n",
        "    xc = -1 + 2*torch.rand((nc, 1), generator=keys[1])\n",
        "    tc = 0.5*torch.rand((nc, 1), generator=keys[0])\n",
        "    xc_mesh, tc_mesh = torch.meshgrid(xc.ravel(), tc.ravel(), indexing='ij')\n",
        "\n",
        "    # initial points\n",
        "    ti = torch.zeros((1, 1))\n",
        "    xi = xc\n",
        "    ui = (xi**2)*torch.cos(torch.pi*xi)\n",
        "\n",
        "\n",
        "    # boundary points (hard-coded)\n",
        "    xb = [-1*torch.ones(1,1).to(device)]  + [1*torch.ones(1,1).to(device)]\n",
        "    tb = 0.5*torch.rand(nc,1)\n",
        "\n",
        "    return xc.to(device), tc.to(device), xi.to(device), ti.to(device), ui.to(device), xb, tb.to(device)\n",
        "\n",
        "def gausspts_train_generator_AC1D(nc):\n",
        "\n",
        "    # Domain Points\n",
        "    xd = torch.linspace(-1, 1, nc)\n",
        "\n",
        "    xleft = xd[:-1]\n",
        "    xright = xd[1:]\n",
        "    xg = torch.zeros(((2*(nc-1)),1))\n",
        "    xg1 = -(1/3**0.5)*(xright - xleft)/2 + (xright + xleft)/2\n",
        "    xg2 = (1/3**0.5)*(xright - xleft)/2 + (xright + xleft)/2\n",
        "    xg[0:-1:2,0] = xg1\n",
        "    xg[1::2,0] = xg2\n",
        "\n",
        "    xb = [-1*torch.ones(1).to(device)]  + [1*torch.ones(1).to(device)]\n",
        "    h = (xd[2] - xd[1])/2\n",
        "    return xg.to(device), xb, h.to(device), xd.to(device)\n",
        "\n",
        "def icgl_train_generator_AC1D(nc):\n",
        "\n",
        "    xd  = torch.linspace(-1, 1, nc)\n",
        "    xleft = xd[:-1]\n",
        "    xright = xd[1:]\n",
        "    xg = torch.zeros(((2*(nc-1)),1))\n",
        "    xg1 = -(1/3**0.5)*(xright - xleft)/2 + (xright + xleft)/2\n",
        "    xg2 = (1/3**0.5)*(xright - xleft)/2 + (xright + xleft)/2\n",
        "    xg[0:-1:2,0] = xg1\n",
        "    xg[1::2,0] = xg2\n",
        "    xi = xg\n",
        "\n",
        "    ui = (xi**2)*torch.cos(torch.pi*xi)*torch.exp(-xi**2)\n",
        "    #ui = torch.sin(xi*torch.pi)\n",
        "\n",
        "    return xi.to(device), ui.to(device)"
      ],
      "metadata": {
        "id": "XBMzn78vi58W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##############################################################\n",
        "# Training functions and utilities\n",
        "\n",
        "def train_step(loss_fn,optimizer,epoch, lossVal, sol_list, tau, train_data_gauss, train_data_icgl):\n",
        "    # clear the gradients\n",
        "    optimizer.zero_grad()\n",
        "    # Loss\n",
        "    loss_spinn, ener_loss, bound_loss, moving_loss = loss_fn(spinn, spinn, tau, train_data_gauss, train_data_icgl)\n",
        "    loss_value = loss_spinn.detach().cpu().numpy()\n",
        "    lossVal.append(loss_value)\n",
        "    _, _, _, xd = train_data_gauss\n",
        "    sol = spinn(xd.reshape(xd.shape[0],1))\n",
        "    sol_list.append(sol)\n",
        "    end_time = time.time()\n",
        "    if epoch%1000 == 0:\n",
        "        print('Energy Loss:',ener_loss.detach().cpu().numpy(),', Bound Loss:',bound_loss.detach().cpu().numpy(),', Moving Loss:',moving_loss.detach().cpu().numpy(),', Total Loss:',loss_value, ', iter:', epoch)\n",
        "    loss_spinn.backward()\n",
        "    # Update model weights\n",
        "    optimizer.step()\n",
        "    return loss_spinn\n",
        "\n",
        "def train_step_icgl(loss_fn,optimizer,epoch,train_data_icgl):\n",
        "    # clear the gradients\n",
        "    optimizer.zero_grad()\n",
        "    # Loss\n",
        "    loss_ic = loss_fn(spinn, train_data_icgl)\n",
        "    loss_value = loss_ic.detach().cpu().numpy()\n",
        "    end_time = time.time()\n",
        "    if epoch%1000 == 0:\n",
        "        print(' Total Loss:',loss_value, ', iter:', epoch)\n",
        "    loss_ic.backward()\n",
        "    # Update model weights\n",
        "    optimizer.step()\n",
        "    return loss_ic\n",
        "\n",
        "\n",
        "def closure():\n",
        "    # Zero gradients\n",
        "    lbfgs.zero_grad()\n",
        "\n",
        "    # Compute loss\n",
        "    loss, ener_loss, bound_loss, init_loss = spinn_loss(spinn, spinn, *train_data_gauss)\n",
        "\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "\n",
        "    return loss, ener_loss, bound_loss, init_loss\n",
        "\n",
        "def repackage_hidden(h):\n",
        "    \"\"\"Wraps hidden states in new Variables, to detach them from their history.\"\"\"\n",
        "    if type(h) == Variable:\n",
        "        return Variable(h.data)\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)\n"
      ],
      "metadata": {
        "id": "Mn-iyH32jHsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##############################################################\n",
        "# Main Code\n",
        "\n",
        "# random key\n",
        "g_cpu = torch.Generator()\n",
        "keys =  [g_cpu.manual_seed(SEED),g_cpu.manual_seed(SEED),g_cpu.manual_seed(SEED)]\n",
        "\n",
        "\n",
        "# dataset\n",
        "\n",
        "nc = 512 # user input\n",
        "train_data = spinn_train_generator_AC1D(nc, keys)\n",
        "train_data_gauss = gausspts_train_generator_AC1D(nc)\n",
        "train_data_icgl = icgl_train_generator_AC1D(nc)\n",
        "\n",
        "\n",
        "# User Input for Size of Neural Network\n",
        "input_size = 1  # You can change this to the desired number of input features\n",
        "hidden_sizes = [64, 64, 64, 64, 64, 64]  # You can specify the number of neurons in each hidden layer\n",
        "output_size = 1\n",
        "epochs_icgl = 1001\n",
        "epochs_pinn = 1001\n",
        "tau = 0.00001/2\n",
        "activation = 'gelu' # Choose either tanh or gelu\n",
        "N = 512\n",
        "xgrid = torch.linspace(-1, 1, N).to(device)\n",
        "t = 0\n",
        "\n",
        "# Create an instance of the neural network\n",
        "spinn = NeuralNetwork(input_size,hidden_sizes,output_size, activation).to(device)\n",
        "# spinn = torch.jit.script(spinn).to(device)\n",
        "# spinn.apply(init_weights)\n",
        "\n",
        "\n",
        "# Define an optimizer\n",
        "adam = optim.Adam(spinn.parameters(),lr=0.001)  # You can adjust the learning rate (lr) as needed\n",
        "scheduler = lr_scheduler.LinearLR(adam,start_factor=1,end_factor=0.1,total_iters=epochs_pinn)\n",
        "lbfgs = optim.LBFGS(spinn.parameters(), history_size=4, max_iter=10)\n",
        "lossVal = []\n",
        "sol_list = []\n",
        "upred = []\n",
        "\n",
        "\n",
        "start = time.time()\n",
        "# Training with ADAM for Initial Condition\n",
        "for epoch in range(epochs_icgl):\n",
        "        start_time = time.time()\n",
        "        loss_fn = icgl_loss\n",
        "        train_step_icgl(loss_fn,adam,epoch,train_data_icgl)\n",
        "\n",
        "upred.append(spinn(xgrid.reshape(N,1)))\n",
        "# Training with ADAM\n",
        "for epoch in range(epochs_pinn):\n",
        "        start_time = time.time()\n",
        "        loss_fn = spinn_loss\n",
        "        train_step(loss_fn,adam,epoch, lossVal, sol_list,tau,train_data_gauss, train_data_icgl)\n",
        "        scheduler.step()\n",
        "\n",
        "upred.append(spinn(xgrid.reshape(N,1)))\n",
        "t += tau\n",
        "print('Sim Time: ', t)\n",
        "\n",
        "for i in range(20):\n",
        "    xi, ui = train_data_icgl\n",
        "    ui = spinn(xi)\n",
        "    ui.detach_()\n",
        "    train_data_icgl = xi,ui\n",
        "\n",
        "    # Training with ADAM\n",
        "    for epoch in range(epochs_pinn):\n",
        "            start_time = time.time()\n",
        "            loss_fn = spinn_loss\n",
        "            train_step(loss_fn,adam,epoch, lossVal, sol_list,tau,train_data_gauss, train_data_icgl)\n",
        "            scheduler.step()\n",
        "\n",
        "\n",
        "    upred.append(spinn(xgrid.reshape(N,1)))\n",
        "    t += tau\n",
        "    print('Sim Time: ', t)\n",
        "\n",
        "end = time.time()\n",
        "print('Total time taken for training: ',end-start)"
      ],
      "metadata": {
        "id": "xaHwJHjTjY9J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "outputId": "f6dbeed8-5653-4356-c081-e51b566675c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-1867f21fd66d>\u001b[0m in \u001b[0;36m<cell line: 46>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0micgl_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mtrain_step_icgl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0madam\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_data_icgl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0mupred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspinn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-0eacf3e70034>\u001b[0m in \u001b[0;36mtrain_step_icgl\u001b[0;34m(loss_fn, optimizer, epoch, train_data_icgl)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# Loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mloss_ic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspinn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_icgl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_ic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-2e33a23dacf2>\u001b[0m in \u001b[0;36micgl_loss\u001b[0;34m(apply_fn, train_data_icgl)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0micgl_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapply_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_icgl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data_icgl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapply_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-05a122702b35>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LBFGS(Optional)\n",
        "# Training with LBFGS\n",
        "for epoch in range(10000):\n",
        "        running_loss = 0.0\n",
        "        # Update weights\n",
        "        lbfgs.step(closure)\n",
        "        # Update the running loss\n",
        "        loss, ener_loss, bound_loss, init_loss = closure()\n",
        "        running_loss += loss.item()\n",
        "        if epoch%100 == 0:\n",
        "            print('Energy Loss:',ener_loss.detach().numpy(),', Bound Loss:',bound_loss.detach().numpy(),', Initial Loss:',init_loss.detach().numpy(),', Total Loss:',loss, ', iter:', epoch)\n",
        "        if running_loss <= 1e-5:\n",
        "            exit"
      ],
      "metadata": {
        "id": "IrnimxSYjoXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Saving Utilities"
      ],
      "metadata": {
        "id": "0Rjhqs_BkNaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the Model\n",
        "filename = 'AC_withConstraintGELU_Fulltime.pt'\n",
        "checkpt = {'model_params':spinn.state_dict(),\n",
        "                    'optimizer':adam.state_dict(),\n",
        "                    'net': spinn\n",
        "                    }\n",
        "torch.save(checkpt,filename)"
      ],
      "metadata": {
        "id": "na4UtP5JjxgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the solution data to mat file\n",
        "u_pred = []\n",
        "for u in upred:\n",
        "    u_pred.append(u.detach().numpy())\n",
        "\n",
        "umat = {'u': u_pred}\n",
        "scipy.io.savemat('AC_DeepRitz1D_oldeqn.mat',umat)"
      ],
      "metadata": {
        "id": "H5Fv6MD3j-aV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plotting Utilities"
      ],
      "metadata": {
        "id": "-y-oObnfkZtU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#%% Plotting the results\n",
        "def plot_ac(xtrain,uexact,upred,t_val):\n",
        "  plt.figure()\n",
        "  plt.scatter(xtrain,uexact,label='exact')\n",
        "  plt.scatter(xtrain,upred.detach().numpy(),label='predicted')\n",
        "  plt.legend(loc=1)\n",
        "  plt.title(f\"t = {t_val}\")\n",
        "  plt.show()\n",
        "\n",
        "def plot_ac_cmap(x,t,uexact,upred):\n",
        "    tmesh,xmesh = np.meshgrid(t,x)\n",
        "    upred = upred.detach().numpy()\n",
        "    plt.figure()\n",
        "    plt.pcolor(tmesh,xmesh,upred,cmap='coolwarm')\n",
        "    plt.title('Predicted')\n",
        "    plt.figure()\n",
        "    plt.pcolor(tmesh,xmesh,uexact,cmap='coolwarm')\n",
        "    plt.title('Exact')\n",
        "\n",
        "\n",
        "t_val = 0.5 #change this variable based on the time snapshot\n",
        "\n",
        "# Chebfun Solution\n",
        "data = scipy.io.loadmat('AC_R_1.mat')\n",
        "x = data['x']\n",
        "t = data['tt']\n",
        "u = data['uu']\n",
        "uexact = u[:,int(t_val*200)]\n",
        "\n",
        "\n",
        "# Neural Network Prediction\n",
        "xtest = torch.tensor(x.T,dtype=torch.float32)\n",
        "ttest = t_val*torch.ones(1,1)\n",
        "upred = spinn(xtest,ttest)\n",
        "\n",
        "plot_ac(x,uexact,upred,t_val)\n",
        "\n",
        "# For 2D surface plot\n",
        "ttest = torch.tensor(t.T[:int(t_val*200)],dtype=torch.float32)\n",
        "upred = spinn(xtest,ttest)\n",
        "plot_ac_cmap(x,t.T[:int(t_val*200)],u[:,:int(t_val*200)],upred)\n"
      ],
      "metadata": {
        "id": "2nBeSBW0kGRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting results for Deep Ritz Testing\n",
        "step = 20\n",
        "N = 512\n",
        "x = torch.linspace(-1, 1, N).resize(N,1)\n",
        "ytrue = x**2*torch.cos(torch.pi*x)\n",
        "ytrue = ytrue.detach().numpy()\n",
        "ypred = upred[step]\n",
        "\n",
        "rmse = np.sqrt(np.sum((ypred.detach().numpy() - ytrue)**2)/N)\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(x.detach().numpy(), ytrue, label ='IC')\n",
        "plt.scatter(x.detach().numpy(), ypred.detach().numpy(), label ='Predicted')\n",
        "plt.legend(loc = 1)\n",
        "plt.title('rmse = %.6f' %rmse)"
      ],
      "metadata": {
        "id": "WtZly8lHkgKi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}